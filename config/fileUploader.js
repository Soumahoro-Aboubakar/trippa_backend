
import crypto from 'crypto';
import zlib from 'zlib';
import fs from 'fs-extra';
import path from 'path';
import { fileURLToPath } from 'url';
import { handleMediaUpload } from '../services/mediaService.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);


const UPLOAD_DIR = path.join(__dirname, 'uploads');
const ENCRYPTION_KEY = process.env.ENCRYPTION_KEY || 'u4Vb7x2MTZ3qYpnDfKLaX1gRhPBwEc5s'; // 32 caract√®res
const MAX_CHUNK_SIZE = parseInt(process.env.MAX_CHUNK_SIZE) || 2 * 1024 * 1024;     // 2 * 1024 * 1024; // 2MB max par chunk
const MAX_FILE_SIZE =  parseInt(process.env.MAX_FILE_SIZE) || 100 * 1024 * 1024;    ///100 * 1024 * 1024; // 100MB max par fichier

// Assurer que le dossier uploads existe
await fs.ensureDir(UPLOAD_DIR);

// Gestionnaire de sessions d'upload
class UploadManager {
  constructor() {
    this.sessions = new Map();
    // D√©marrer le nettoyage automatique
    this.startAdvancedCleanup();
        this.completedFiles = new Map(); // Tracker les fichiers termin√©s
  }

  createSession(fileId, userId, expectedChunks, totalSize, fileName, mimeType) {
    const session = {
      fileId,
      userId,
      expectedChunks,
      totalSize,
      fileName,
      mimeType,
      receivedChunks: new Set(),
      chunkData: new Map(),
      createdAt: new Date(),
      lastActivity: new Date()
    };
    
    this.sessions.set(fileId, session);
    return session;
  }

  getSession(fileId) {
    return this.sessions.get(fileId);
  }

  addChunk(fileId, chunkIndex, chunkData) {
    const session = this.sessions.get(fileId);
    if (!session) return false;

    session.receivedChunks.add(chunkIndex);
    if (chunkData) {
      session.chunkData.set(chunkIndex, chunkData);
    }
    session.lastActivity = new Date();
    
    return true;
  }

  isComplete(fileId) {
    const session = this.sessions.get(fileId);
    if (!session) return false;
    
    return session.receivedChunks.size === session.expectedChunks;
  }

 removeSession(fileId) {
    const session = this.sessions.get(fileId);
    if (session) {
      console.log(`Suppression de la session: ${fileId}`);
      this.sessions.delete(fileId);
    }
  }


 /* cleanup() {
    const now = new Date();
    const TIMEOUT = 30 * 60 * 1000; // 30 minutes

    for (const [fileId, session] of this.sessions.entries()) {
      if (now - session.lastActivity > TIMEOUT) {
        this.removeSession(fileId);
        // Nettoyer les fichiers temporaires
        this.cleanupTempFiles(fileId).catch(console.error);
      }
    }
  }*/


  // Nouveau: Marquer un fichier comme termin√© avec succ√®s
  markFileCompleted(fileId, filePath) { // le path ici est en rapport avec l'ur du fichier ici tous reussi (backbaze)
    this.completedFiles.set(fileId, {
      path: filePath,
      completedAt: new Date(),
      fileId: fileId
    });
    console.log(`File marked as completed: ${fileId} at ${filePath}`);
  }



  
  startAdvancedCleanup() {
    // Timer 1: Nettoyer les fichiers termin√©s toutes les heures
    setInterval(() => {
      this.cleanupCompletedFiles();
    }, 60 * 60 * 1000); // 1 heure

    // Timer 2: Nettoyer tous les fichiers obsol√®tes toutes les 4 heures  
    setInterval(() => {
      this.cleanupAllObsoleteFiles();
    }, 4 * 60 * 60 * 1000); // 4 heures

    console.log('Advanced cleanup timers started');
  }

   // Nettoyage des fichiers termin√©s depuis 1h+
  async cleanupCompletedFiles() {
    const now = new Date();
    const ONE_HOUR = 60 * 60 * 1000;
    let cleanedCount = 0;

    console.log('Starting cleanup of completed files (1h+)...');
    
    for (const [fileId, fileInfo] of this.completedFiles.entries()) {
      const fileAge = now - fileInfo.completedAt;
      
      if (fileAge > ONE_HOUR) {
        try {
          // Supprimer le fichier du syst√®me
          if (fileInfo.path && await fs.pathExists(fileInfo.path)) {
            await fs.remove(fileInfo.path);
            console.log(`Deleted completed file: ${fileInfo.path}`);
          }
          
          // Supprimer de la m√©moire
          this.completedFiles.delete(fileId);
          this.removeSession(fileId);
          // Supprimer dossier temp si existe encore
          await this.cleanupTempFiles(fileId); //A revoir  car ici la section n'est pas √©ffacer
          
          cleanedCount++;
          
        } catch (error) {
          console.error(`Failed to cleanup completed file ${fileId}:`, error);
        }
      }
    } console.log(`Completed files cleanup: ${cleanedCount} files removed`);
  }
   // Nettoyage de tous les fichiers obsol√®tes depuis 4h+
  async cleanupAllObsoleteFiles() {
    const now = new Date();
    const FOUR_HOURS = 4 * 60 * 60 * 1000;
    let cleanedSessions = 0;
    let cleanedFiles = 0;
    let cleanedDirs = 0;

    console.log('Starting cleanup of all obsolete files (4h+)...');

    // 1. Nettoyer les sessions actives obsol√®tes (annul√©es, en attente, etc.)
    for (const [fileId, session] of this.sessions.entries()) {
      const sessionAge = now - session.createdAt;
      
      if (sessionAge > FOUR_HOURS) {
        try {
          await this.cleanupTempFiles(fileId);
          this.removeSession(fileId);
          cleanedSessions++;
          console.log(`Cleaned obsolete session: ${fileId}`);
        } catch (error) {
          console.error(`Failed to cleanup session ${fileId}:`, error);
        }
      }
    }

    // 2. Nettoyer les fichiers termin√©s obsol√®tes
    for (const [fileId, fileInfo] of this.completedFiles.entries()) {
      const fileAge = now - fileInfo.completedAt;
      
      if (fileAge > FOUR_HOURS) {
        try {
          if (fileInfo.path && await fs.pathExists(fileInfo.path)) {
            await fs.remove(fileInfo.path);
          }
          this.completedFiles.delete(fileId);
          await this.cleanupTempFiles(fileId);
          cleanedFiles++;
          console.log(`Cleaned obsolete completed file: ${fileId}`);
        } catch (error) {
          console.error(`Failed to cleanup obsolete file ${fileId}:`, error);
        }
      }
    }

    // 3. Nettoyer les dossiers orphelins
    cleanedDirs = await this.cleanupOrphanDirectories(FOUR_HOURS);

    console.log(`All obsolete files cleanup: ${cleanedSessions} sessions, ${cleanedFiles} files, ${cleanedDirs} directories removed`);
  }
 // Nettoyer les dossiers orphelins dans UPLOAD_DIR
  async cleanupOrphanDirectories(maxAge) {
    let cleanedCount = 0;
    
    try {
      if (!await fs.pathExists(UPLOAD_DIR)) {
        return cleanedCount;
      }

      const items = await fs.readdir(UPLOAD_DIR);
      
      for (const item of items) {
        const itemPath = path.join(UPLOAD_DIR, item);
        
        try {
          const stats = await fs.stat(itemPath);
          
          if (stats.isDirectory()) {
            const isTracked = this.sessions.has(item) || this.completedFiles.has(item);
            const itemAge = Date.now() - stats.mtime.getTime();
            
            // Supprimer si pas suivi ET trop vieux
            if (!isTracked && itemAge > maxAge) {
              await fs.remove(itemPath);
              cleanedCount++;
              console.log(`Removed orphan directory: ${itemPath}`);
            }
          }
        } catch (error) {
          console.error(`Error processing ${itemPath}:`, error);
        }
      }
    } catch (error) {
      console.error('Error during orphan cleanup:', error);
    }
    
    return cleanedCount;
  }
 // Am√©lioration de la m√©thode existante cleanupTempFiles
  async cleanupTempFiles(fileId) {
    const chunkDir = path.join(UPLOAD_DIR, fileId);
    try {
      if (await fs.pathExists(chunkDir)) {
        await fs.remove(chunkDir);
        console.log(`Cleaned temp files for: ${fileId}`);
      }
    } catch (error) {
      console.error(`Failed to cleanup temp files for ${fileId}:`, error);
    }
  }
  // Statistiques pour monitoring
  getCleanupStats() {
    const now = new Date();
    const ONE_HOUR = 60 * 60 * 1000;
    const FOUR_HOURS = 4 * 60 * 60 * 1000;

    let completedReadyForCleanup = 0;
    let sessionsReadyForCleanup = 0;

    // Compter les fichiers termin√©s pr√™ts pour nettoyage (1h+)
    for (const [, fileInfo] of this.completedFiles.entries()) {
      if (now - fileInfo.completedAt > ONE_HOUR) {
        completedReadyForCleanup++;
      }
    }

    // Compter les sessions pr√™tes pour nettoyage (4h+)
    for (const [, session] of this.sessions.entries()) {
      if (now - session.createdAt > FOUR_HOURS) {
        sessionsReadyForCleanup++;
      }
    }

    return {
      activeSessions: this.sessions.size,
      completedFiles: this.completedFiles.size,
      completedReadyForCleanup,
      sessionsReadyForCleanup,
      totalTracked: this.sessions.size + this.completedFiles.size
    };
  }



}

// Instance globale du gestionnaire d'upload
const uploadManager = new UploadManager();

function decryptChunk(encryptedData, encryptionKey) {
  try {
    console.log(`Tentative de d√©chiffrement: ${encryptedData.length} bytes`);
    
    // Cr√©er la cl√© de 32 bytes exactement comme dans Flutter
    const keyBuffer = Buffer.alloc(32);
    const keyBytes = Buffer.from(encryptionKey, 'utf8');
    
    // Copier les bytes de la cl√© et remplir avec des z√©ros
    for (let i = 0; i < 32; i++) {
      if (i < keyBytes.length) {
        keyBuffer[i] = keyBytes[i];
      } else {
        keyBuffer[i] = 0;
      }
    }
    
    console.log(`Cl√© g√©n√©r√©e: ${keyBuffer.toString('hex')}`);
    
    // IV de 16 bytes tous √† z√©ro (comme dans Flutter)
    const iv = Buffer.alloc(16, 0);
    
    console.log(`IV utilis√©: ${iv.toString('hex')}`);
    
    // Cr√©er le d√©chiffreur
    const decipher = crypto.createDecipheriv('aes-256-cbc', keyBuffer, iv);
    decipher.setAutoPadding(true);
    
    // D√©chiffrer
    const decrypted = Buffer.concat([
      decipher.update(encryptedData),
      decipher.final()
    ]);
    
    console.log(`D√©chiffrement r√©ussi: ${decrypted.length} bytes`);
    return decrypted;
    
  } catch (error) {
    console.error(`Erreur de d√©chiffrement d√©taill√©e:`, {
      message: error.message,
      code: error.code,
      encryptedDataLength: encryptedData.length,
      keyLength: encryptionKey.length
    });
    throw new Error(`Decryption failed: ${error.message}`);
  }
}

// Fonction de d√©compression avec gestion d'erreur am√©lior√©e
function decompressChunk(compressedData) {
  try {
    console.log(`Tentative de d√©compression: ${compressedData.length} bytes`);
    const decompressed = zlib.inflateSync(compressedData);
    console.log(`D√©compression r√©ussie: ${decompressed.length} bytes`);
    return decompressed;
  } catch (error) {
    console.error(`Erreur de d√©compression:`, {
      message: error.message,
      compressedDataLength: compressedData.length,
      firstBytes: compressedData.slice(0, 16).toString('hex')
    });
    throw new Error(`Decompression failed: ${error.message}`);
  }
}

// Fonction de validation du hash am√©lior√©e
function calculateHash(data) {
  const hash = crypto.createHash('sha256').update(data).digest('hex');
  console.log(`Hash calcul√©: ${hash}`);
  return hash;
}


// Validation des inputs
function validateChunkInput(data) {
  const required = ['index', 'fileId', 'data', 'userId', 'size', 'hash'];
  
  for (const field of required) {
    if (!(field in data)) {
      throw new Error(`Missing required field: ${field}`);
    }
  }

  // Validation des types
  if (typeof data.index !== 'number' || data.index < 0) {
    throw new Error('Invalid chunk index');
  }

  if (typeof data.size !== 'number' || data.size <= 0 || data.size > MAX_CHUNK_SIZE) {
    throw new Error('Invalid chunk size');
  }

  if (typeof data.fileId !== 'string' || !/^[a-zA-Z0-9_-]+$/.test(data.fileId)) {
    throw new Error('Invalid fileId format');
  }

  if (typeof data.userId !== 'string' || data.userId.length === 0) {
    throw new Error('Invalid userId');
  }

  if (typeof data.hash !== 'string' || !/^[a-f0-9]{64}$/.test(data.hash)) {
    throw new Error('Invalid hash format');
  }

  return true;
}

// Fonction pour obtenir le statut d'une session
async function getSessionStatus(fileId) {
  const session = uploadManager.getSession(fileId);

  if (!session) {
    throw new Error('Upload session not found');
  }

  // V√©rifier quels chunks existent d√©j√† sur le disque
  const chunkDir = path.join(UPLOAD_DIR, fileId);
  const existingChunks = [];
  
  try {
    if (await fs.pathExists(chunkDir)) {
      const files = await fs.readdir(chunkDir);
      for (const file of files) {
        const match = file.match(/^chunk_(\d+)$/);
        if (match) {
          existingChunks.push(parseInt(match[1]));
        }
      }
    }
  } catch (error) {
    console.error('Error checking existing chunks:', error);
    throw new Error('Error checking existing chunks');
  }

  return {
    fileId,
    expectedChunks: session.expectedChunks,
    receivedChunks: Array.from(session.receivedChunks),
    existingChunks: existingChunks.sort((a, b) => a - b),
    isComplete: uploadManager.isComplete(fileId),
    totalSize: session.totalSize,
    fileUploadedOnBackbazeId :session.path,
  };
}

// Fonction pour reconstruire le fichier final
async function reconstructFile(fileId, session) {
  const chunkDir = path.join(UPLOAD_DIR, fileId);
  const chunks = [];
  
  // Lire tous les chunks dans l'ordre
  for (let i = 0; i < session.expectedChunks; i++) {
    const chunkPath = path.join(chunkDir, `chunk_${i}`);
    
    if (!await fs.pathExists(chunkPath)) {
      throw new Error(`Missing chunk ${i}`);
    }
    
    const chunkData = await fs.readFile(chunkPath);
    chunks.push(chunkData);
  }
  
  // Concat√©ner tous les chunks
  return Buffer.concat(chunks);
}


// Fonction principale pour configurer les √©v√©nements Socket.IO
export const mediaUploader = async (socket) => {
  // Initialisation d'une session d'upload
 socket.on('upload_init', async (data) => {
  try {
    const { fileId, userId, expectedChunks, totalSize, fileName, mimeType } = data;
    
    // Validation
    if (!fileId || !userId || !expectedChunks || !totalSize) {
      socket.emit('upload_error', { 
        fileId, 
        error: 'Missing required fields for upload initialization' 
      });
      return;
    }

    if (totalSize > MAX_FILE_SIZE) {
      socket.emit('upload_error', { 
        fileId, 
        error: 'File size exceeds maximum allowed size' 
      });
      return;
    }

    // NOUVEAU: V√©rifier si le fichier a d√©j√† √©t√© compl√©t√©
    const completedFile = uploadManager.completedFiles.get(fileId);
    if (completedFile) {
      console.log(`Fichier d√©j√† compl√©t√© lors de l'init: ${fileId}`);
      socket.emit('upload_initialized', { 
        fileId, 
        fileUploadedOnBackbazeId : completedFile.path,
   //     result : completedFile,
        existingChunks: Array.from({length: expectedChunks}, (_, i) => i),
        alreadyCompleted: true,
        message: 'File already completed' 
      });
      return;
    }

    // V√©rifier si une session existe d√©j√†
    let session = uploadManager.getSession(fileId);
    if (session) {
      console.log(`Session existante trouv√©e pour: ${fileId}`);
    } else {
      // Cr√©er nouvelle session
      session = uploadManager.createSession(fileId, userId, expectedChunks, totalSize, fileName, mimeType);
    }
    
    // Cr√©er le dossier pour les chunks
    const chunkDir = path.join(UPLOAD_DIR, fileId);
    await fs.ensureDir(chunkDir);

    // V√©rifier les chunks d√©j√† pr√©sents
    const existingChunks = [];
    try {
      const files = await fs.readdir(chunkDir);
      for (const file of files) {
        const match = file.match(/^chunk_(\d+)$/);
        if (match) {
          const chunkIndex = parseInt(match[1]);
          existingChunks.push(chunkIndex);
          uploadManager.addChunk(fileId, chunkIndex, null);
        }
      }
    } catch (error) {
      console.error('Error checking existing chunks:', error);
    }

    socket.emit('upload_initialized', { 
      fileId, 
      existingChunks: existingChunks.sort((a, b) => a - b),
      message: 'Upload session initialized successfully' 
    });

  } catch (error) {
    console.error('Upload init error:', error);
    socket.emit('upload_error', { 
      fileId: data.fileId, 
      error: error.message 
    });
  }
});

  async function handleUploadCompletion(socket, fileId, session) {
  try {
    const finalBuffer = await reconstructFile(fileId, session);
    
    const mediaResult = await handleMediaUpload(socket, {
      buffer: [finalBuffer],
      fileName: session.fileName || `${fileId}.bin`,
      mimeType: session.mimeType || 'application/octet-stream',
      mediaDuration:  null //A revoir
    });

    if (mediaResult && mediaResult.mediaPath) {
      uploadManager.markFileCompleted(fileId, mediaResult.mediaPath);
    }
    
    socket.emit('upload_complete', {
      fileId,
      success: true,
      result: mediaResult
    });

   
  } catch (error) {
    console.error('Erreur de reconstruction/upload:', error);
    socket.emit('upload_complete', {
      fileId,
      success: false,
      error: error.message
    });
  }
}


// 1. Am√©liorer la gestion des chunks d√©j√† re√ßus
socket.on('file_chunk', async (data) => {
  try {
    console.log(`\n=== Traitement du chunk ${data.index} ===`);
    console.log(`FileId: ${data.fileId}`);
    
    // Validation des inputs
    validateChunkInput(data);
    
    const { index, fileId, data: encodedChunk, userId, size, hash } = data;
      // SOLUTION 1: V√©rifier si le fichier a d√©j√† √©t√© compl√©t√©
      const completedFile = uploadManager.completedFiles.get(fileId);
    
    // V√©rifier la session
    const session = uploadManager.getSession(fileId);
    if (!session) {
      console.log(`Session non trouv√©e pour fileId: ${fileId}`);
      
      if (completedFile) {
        console.log(`Fichier d√©j√† compl√©t√©: ${fileId}`);
        socket.emit('chunk_received', {
          fileId,
          index,
          success: true,
          message: 'File already completed',
          alreadyCompleted: true
        });
        
        // Envoyer aussi upload_complete pour informer le client
        socket.emit('upload_complete', {
          fileId,
          success: true,
          result: completedFile,
          message: 'File was already uploaded',
          alreadyCompleted: true
        });
        return;
      }
      
      // Si vraiment aucune session trouv√©e
      socket.emit('chunk_received', {
        fileId,
        index,
        success: false,
        error: 'No active upload session found. Please reinitialize upload.'
      });
      return;
    }

    // V√©rifier que le chunk n'a pas d√©j√† √©t√© re√ßu
    if (session.receivedChunks.has(index)) {
      console.log(`Chunk ${index} d√©j√† re√ßu`);
      socket.emit('chunk_received', {
        fileId,
        index,
        success: true,
        message: 'Chunk already received'
      });
      
      // SOLUTION 2: V√©rifier si l'upload est d√©j√† termin√©
      if (uploadManager.isComplete(fileId)) {
        socket.emit('upload_complete', {
          fileId,
          result: completedFile,
          success: true,
          message: 'Upload already completed'
        });
      }
      return;
    }

    // V√©rifier si le chunk existe d√©j√† sur le disque
    const chunkPath = path.join(UPLOAD_DIR, fileId, `chunk_${index}`);
    if (await fs.pathExists(chunkPath)) {
      console.log(`Chunk ${index} existe d√©j√† sur le disque`);
      
      // Marquer comme re√ßu si pas d√©j√† fait
      uploadManager.addChunk(fileId, index, null);
      
      socket.emit('chunk_received', {
        fileId,
        index,
        success: true,
        message: 'Chunk already exists on disk'
      });
      
      // V√©rifier si l'upload est termin√©
      if (uploadManager.isComplete(fileId)) {
        console.log(`Upload termin√© pour le fichier: ${fileId}`);
        await handleUploadCompletion(socket, fileId, session);
      }
      return;
    }

    // Traitement normal du chunk...
    console.log(`D√©codage base64: ${encodedChunk.length} caract√®res`);
    const encryptedData = Buffer.from(encodedChunk, 'base64');
    console.log(`Donn√©es chiffr√©es: ${encryptedData.length} bytes`);
    
    // D√©chiffrer
    const compressedData = decryptChunk(encryptedData, ENCRYPTION_KEY);
    
    // D√©compresser
    const originalData = decompressChunk(compressedData);
    
    // V√©rifier la taille
    if (originalData.length !== size) {
      console.error(`Erreur de taille: attendu ${size}, re√ßu ${originalData.length}`);
      socket.emit('chunk_received', {
        fileId,
        index,
        success: false,
        error: `Size mismatch: expected ${size}, got ${originalData.length}`
      });
      return;
    }
    
    // V√©rifier le hash
    const calculatedHash = calculateHash(originalData);
    if (calculatedHash !== hash) {
      console.error(`Erreur de hash: attendu ${hash}, calcul√© ${calculatedHash}`);
      socket.emit('chunk_received', {
        fileId,
        index,
        success: false,
        error: 'Hash verification failed'
      });
      return;
    }
    
    // Sauvegarder le chunk
    await fs.writeFile(chunkPath, originalData);
    console.log(`Chunk ${index} sauvegard√©: ${chunkPath}`);
    
    // Marquer le chunk comme re√ßu
    uploadManager.addChunk(fileId, index, originalData);
    
    // R√©pondre au client
    socket.emit('chunk_received', {
      fileId,
      index,
      success: true
    });

    console.log(`=== Chunk ${index} trait√© avec succ√®s ===\n`);

    // V√©rifier si l'upload est termin√©
    if (uploadManager.isComplete(fileId)) {
      console.log(`Upload termin√© pour le fichier: ${fileId}`);
      await handleUploadCompletion(socket, fileId, session);
    }

  } catch (error) {
    console.error(`Erreur de traitement du chunk ${data.index}:`, error);
    socket.emit('chunk_received', {
      fileId: data.fileId,
      index: data.index,
      success: false,
      error: error.message
    });
  }
});



  // Obtenir le statut d'une session
  socket.on('get_upload_status', async (data) => {
    try {
      const { fileId } = data;
      const status = await getSessionStatus(fileId);
      console.log(`Statut de l'upload pour fileId ${fileId}:`, status);
      socket.emit('upload_status', status);
    } catch (error) {
      socket.emit('upload_error', {
        fileId: data.fileId,
        error: error.message
      });
    }
  });

  // Annuler un upload
  socket.on('cancel_upload', async (data) => {
    try {
      const { fileId } = data;
   /*   await uploadManager.cleanupTempFiles(fileId);
      uploadManager.removeSession(fileId);*/
      deleteFileCompletely(fileId);
      socket.emit('upload_cancelled', { fileId });
    } catch (error) {
      socket.emit('upload_error', {
        fileId: data.fileId,
        error: error.message
      });
    }
  });
};


async function deleteFileCompletely(fileId) { //permet d'effacer tous fichier, √ßa particularit√© est qu'elle sera importable dans n'importe quel fichier de l'app
  if(!fileId) return;
  const result = {
    fileId,
    success: true,
    deletedItems: [],
    timestamp: new Date().toISOString()
  };

  console.log(`üóëÔ∏è SUPPRESSION COMPL√àTE DEMAND√âE pour fileId: ${fileId}`);

  function addError(message, error) {
    result.success = false;
    result.deletedItems.push(`${message}: ${error.message}`);
    console.error(message, error);
  }

  try {
    // 1. SUPPRIMER LA SESSION ACTIVE
    const session = uploadManager.getSession(fileId);
    if (session) {
      uploadManager.removeSession(fileId);
      result.deletedItems.push(`Session active supprim√©e`);
      console.log(`‚úÖ Session supprim√©e: ${fileId}`);
    } else {
      result.deletedItems.push(`Aucune session active trouv√©e`);
    }

    // 2. SUPPRIMER LE FICHIER COMPL√âT√â DE LA M√âMOIRE
    uploadManager.cleanupTempFiles(fileId);
    const completedFile = uploadManager.completedFiles.get(fileId);
    if (completedFile) 
       uploadManager.completedFiles.delete(fileId);
    return result;
  } catch (error) {
    addError("Erreur globale lors de la suppression compl√®te", error);
    return result;
  }
}

export { deleteFileCompletely };